# -*- coding: utf-8 -*-
"""deteCT_Model_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/152i4H-gKZAq5xhVDEIDv0zXNQiYasncU
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img

"""# Creating Training and Validation Augmented Dataset"""

def train_val_generators(TRAINING_DIR, VALIDATION_DIR):
  train_datagen = ImageDataGenerator(rescale=1/255,
                                     rotation_range=30,
                                     shear_range=0.2,
                                     brightness_range=((0.2, 0.8))
                                     )
  
  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,
                                                      batch_size=32,
                                                      class_mode='binary',
                                                      target_size=(160, 160))
  validation_datagen = ImageDataGenerator(rescale=1/255,
                                          rotation_range=30,
                                          shear_range=0.2,)
  
  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,
                                                                batch_size=32,
                                                                class_mode='binary',
                                                                target_size=(160, 160))
  
  return train_generator, validation_generator

TRAINING_DIR = "/content/gdrive/My Drive/Project/Dataseto/training/"
TESTING_DIR = "/content/gdrive/My Drive/Project/Dataseto/validation/"
train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)

denseModel = tf.keras.applications.densenet.DenseNet201(input_shape=(160, 160, 3),
                                               include_top=False,
                                               weights='imagenet')

# Getting the last layer of DenseNet201 model
last_output = denseModel.output

fine_tune_at = 600

for layer in denseModel.layers[:fine_tune_at]:
  layer.trainable = False

# denseModel.trainable = False

# Global Average Pooling 2D initialization
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

# x = tf.keras.layers.Flatten()(last_output)
# x = tf.keras.layers.Dense(1024, activation='relu')(x)

# Adding New Layer
x = global_average_layer(last_output)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(1)(x)

# Create model with DenseNet201 Model added with 3 new layers
model = tf.keras.Model(denseModel.input, x)

bekup = model

# Creating Callback Function to Stop Training when Accuracy Reached 95%
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.95):
      print("\naccuracy is higher than 95% so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

# Creating Callback Function to Save Model in Gdrive every 4 epochs
class Save(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      root_path = '/content/gdrive/My Drive/Project/models2/'
      if epoch%4 == 0:  
        path = os.path.join(root_path, str(epoch))
        os.makedirs(path,exist_ok=True)
        self.model.save("/content/gdrive/MyDrive/Project/models2/{}".format(epoch))
saver = Save()

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()

"""# Training Using DenseNet201 Model with Fine Tuning"""

history_transfer = model.fit(train_generator,
                         epochs=41,
                         verbose=1,
                         validation_data=validation_generator,
                         callbacks=[saver])

acc = history_transfer.history['accuracy']
val_acc = history_transfer.history['val_accuracy']

loss = history_transfer.history['loss']
val_loss = history_transfer.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""# Transfer Learning DenseNet201 With Image Size of 480x480 Without Fine Tuning"""

history_transfer = model.fit(train_generator,
                         epochs=40,
                         verbose=1,
                         validation_data=validation_generator,
                         callbacks=[saver])

acc = history_transfer.history['accuracy']
val_acc = history_transfer.history['val_accuracy']

loss = history_transfer.history['loss']
val_loss = history_transfer.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""# Transfer Learning DenseNet201 with Callback To End Training at 95% Accuracy"""

acc = history_transfer.history['accuracy']
val_acc = history_transfer.history['val_accuracy']

loss = history_transfer.history['loss']
val_loss = history_transfer.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

model.save('/content/gdrive/MyDrive/Project/genius')

"""# Test Model"""

from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img = image.load_img(path, target_size=(160, 160))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
  x = x/255

  images = np.vstack([x])
  classes = model.predict(images, batch_size=16)
  print(fn)
  print(classes)

"""# Tried to Continue Learning of Model With Callback 95% Accuracy"""

dingdong = tf.keras.models.load_model('/content/gdrive/MyDrive/Project/genius')

backup = dingdong

history_transfer = dingdong.fit(train_generator,
                         epochs=7,
                         verbose=1,
                         validation_data=validation_generator)

acc = history_transfer.history['accuracy']
val_acc = history_transfer.history['val_accuracy']

loss = history_transfer.history['loss']
val_loss = history_transfer.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img = image.load_img(path, target_size=(480, 480))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = dingdong.predict(images)
  print(fn)
  print(classes)

"""# Convert to TFjs"""

!pip install tensorflowjs

model_load = tf.keras.models.load_model('/content/gdrive/MyDrive/Project/models2/40')

import tensorflowjs as tfjs

tfjs_target_dir = "/content/gdrive/MyDrive/Project/TFjs_Model/"

tfjs.converters.save_keras_model(model_load, tfjs_target_dir)

from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img = image.load_img(path, target_size=(160, 160))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
  x = x/255

  images = np.vstack([x])
  classes = model_load.predict(images)

  if classes < 0:
    classes = 'covid'
    print(classes)
  else:
    classes = 'non-covid'
    print(classes)
  print(fn)